{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Your Data Wrangler Flow to Get Real-Time Predictions\n",
    "\n",
    "Use Amazon SageMaker for real-time inference requests. The following notebook uses the Amazon SageMaker \n",
    "Python SDK to create an inference pipeline.\n",
    "\n",
    "The notebook does the following:\n",
    "1. Creates a SageMaker Pipeline.\n",
    "2. Runs the pipeline.\n",
    "    a. The pipeline uses the loan-default-inf-pipeline-DO-NOT-DEL.flow Data Wrangler flow file to prepare the data. All of the transformations that youâ€™re exporting from your flow file are applied to the entire dataset.\n",
    "    b. The pipeline trains either an Amazon SageMaker Autopilot model or an XGBoost model.\n",
    "3. Creates a SageMaker inference pipeline.\n",
    "4. Deploys the inference pipeline to an endpoint.\n",
    "5. Uses the endpoint to make real-time predictions.\n",
    "    a. The inference pipeline uses the Data Wrangler flow to transform the data from your inference request into a format that the trained model can use.\n",
    "    b. The transformed data is provided to the trained model. The model uses the data to generate real-time predictions.\n",
    "\n",
    "You can use your own predictive model in the notebook. To use your own model, skip the data preparation \n",
    "and model training steps. Navigate to the `Create SageMaker Inference Pipeline` section and specify the Amazon S3 location of the model.\n",
    "\n",
    "For more information about inference pipelines, see [Host models along with pre-processing logic as serial inference pipeline behind one endpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Important</font>\n",
    "\n",
    "<font color='red'>The transforms that youâ€™ve applied to the target column are ignored at the time of inference. The \n",
    "target column is the column that your model is predicting. If your Data Wrangler flow contains a \n",
    "multicolumn transform that applies to the target column and other columns in your dataset, the \n",
    "transform is ignored for all the columns. The pipeline might therefore fail to perform inference.</font>\n",
    "\n",
    "<font color='red'>If your flow has multicolumn transformations that transform the target, modify your Data Wrangler \n",
    "flow to have the transformation to the target column as its own transformation.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install SageMaker Autopilot Dependencies\n",
    "To use SageMaker Autopilot in your pipeline, you must install the newest versions of botocore, boto3, and sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U 'botocore==1.*'\n",
    "!pip install -U 'boto3==1.*'\n",
    "!pip install -U 'sagemaker==2.*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs and Outputs\n",
    "\n",
    "The below settings configure the inputs and outputs for the flow export.\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Configurable Settings </strong>\n",
    "\n",
    "In <b>Input - Source</b> you can configure the data sources that will be used as input by Data Wrangler\n",
    "\n",
    "1. For S3 sources, configure the source attribute that points to the input S3 prefixes\n",
    "2. For all other sources, configure attributes like query_string, database in the source's \n",
    "<b>DatasetDefinition</b> object.\n",
    "\n",
    "If you modify the inputs the provided data must have the same schema and format as the data used in the Flow. \n",
    "You should also re-execute the cells in this section if you have modified the settings in any data sources.\n",
    "\n",
    "Parametrized data sources will be ignored when creating ProcessingInputs, and will directly read from the source.\n",
    "Network isolation is not supported for parametrized data sources.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.dataset_definition.inputs import AthenaDatasetDefinition, DatasetDefinition, RedshiftDatasetDefinition\n",
    "\n",
    "data_sources = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input - S3 Source: loans.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources.append(ProcessingInput(\n",
    "    source=\"s3://sagemaker-us-west-2-251356100815/loan-default/loans.csv\", # You can override this to point to other dataset on S3\n",
    "    destination=\"/opt/ml/processing/loans.csv\",\n",
    "    input_name=\"loans.csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output: S3 settings\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Configurable Settings </strong>\n",
    "\n",
    "1. <b>bucket</b>: you can configure the S3 bucket where Data Wrangler will save the output. The default bucket from \n",
    "the SageMaker notebook session is used. \n",
    "2. <b>flow_export_id</b>: A randomly generated export id. The export id must be unique to ensure the results do not \n",
    "conflict with other flow exports \n",
    "3. <b>s3_ouput_prefix</b>:  you can configure the directory name in your bucket where your data will be saved.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import uuid\n",
    "import sagemaker\n",
    "\n",
    "# Sagemaker session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# You can configure this with your own bucket name, e.g.\n",
    "# bucket = \"my-bucket\"\n",
    "bucket = sess.default_bucket()\n",
    "print(f\"Data Wrangler export storage bucket: {bucket}\")\n",
    "\n",
    "# unique flow export ID\n",
    "flow_export_id = f\"{time.strftime('%d-%H-%M-%S', time.gmtime())}-{str(uuid.uuid4())[:8]}\"\n",
    "flow_export_name = f\"flow-{flow_export_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the inputs required by the SageMaker Python SDK to launch a processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output name is auto-generated from the select node's ID + output name from the flow file.\n",
    "output_name = \"9a9ddd7b-f758-41ff-9b91-68a671a773bb.default\"\n",
    "\n",
    "s3_output_prefix = f\"export-{flow_export_name}/output\"\n",
    "s3_output_base_path = f\"s3://{bucket}/{s3_output_prefix}\"\n",
    "print(f\"Processing output base path: {s3_output_base_path}\\nThe final output location will contain additional subdirectories.\")\n",
    "\n",
    "processing_job_output = ProcessingOutput(\n",
    "    output_name=output_name,\n",
    "    source=\"/opt/ml/processing/output\",\n",
    "    destination=s3_output_base_path,\n",
    "    s3_upload_mode=\"EndOfJob\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Flow to S3\n",
    "\n",
    "To use the Data Wrangler as an input to the processing job,  first upload your flow file to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "# name of the flow file which should exist in the current notebook working directory\n",
    "flow_file_name = \"loan-default-inf-pipeline-DO-NOT-DEL.flow\"\n",
    "\n",
    "# Load .flow file from current notebook working directory \n",
    "!echo \"Loading flow file from current notebook working directory: $PWD\"\n",
    "\n",
    "with open(flow_file_name) as f:\n",
    "    flow = json.load(f)\n",
    "\n",
    "# Upload flow to S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(flow_file_name, bucket, f\"data_wrangler_flows/{flow_export_name}.flow\", ExtraArgs={\"ServerSideEncryption\": \"aws:kms\"})\n",
    "\n",
    "flow_s3_uri = f\"s3://{bucket}/data_wrangler_flows/{flow_export_name}.flow\"\n",
    "\n",
    "print(f\"Data Wrangler flow {flow_file_name} uploaded to {flow_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data Wrangler Flow is also provided to the Processing Job as an input source which we configure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input - Flow: loan-default-inf-pipeline-DO-NOT-DEL.flow\n",
    "flow_input = ProcessingInput(\n",
    "    source=flow_s3_uri,\n",
    "    destination=\"/opt/ml/processing/flow\",\n",
    "    input_name=\"flow\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IAM role for executing the processing job.\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "# Unique processing job name. Give a unique name every time you re-execute processing jobs.\n",
    "processing_job_name = f\"data-wrangler-flow-processing-{flow_export_id}\"\n",
    "\n",
    "# Data Wrangler Container URL.\n",
    "container_uri = \"174368400705.dkr.ecr.us-west-2.amazonaws.com/sagemaker-data-wrangler-container:1.x\"\n",
    "# Pinned Data Wrangler Container URL. \n",
    "container_uri_pinned = \"174368400705.dkr.ecr.us-west-2.amazonaws.com/sagemaker-data-wrangler-container:1.30.2\"\n",
    "\n",
    "# Processing Job Instance count and instance type.\n",
    "instance_count = 2\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "# Size in GB of the EBS volume to use for storing data during processing.\n",
    "volume_size_in_gb = 30\n",
    "\n",
    "# Content type for each output. Data Wrangler supports CSV as default and Parquet.\n",
    "output_content_type = \"CSV\"\n",
    "\n",
    "# Delimiter to use for the output if the output content type is CSV. Uncomment to set.\n",
    "# delimiter = \",\"\n",
    "\n",
    "# Compression to use for the output. Uncomment to set.\n",
    "# compression = \"gzip\"\n",
    "\n",
    "# Configuration for partitioning the output. Uncomment to set.\n",
    "# \"num_partition\" sets the number of partitions/files written in the output.\n",
    "# \"partition_by\" sets the column names to partition the output by.\n",
    "# partition_config = {\n",
    "#     \"num_partitions\": 1,\n",
    "#     \"partition_by\": [\"column_name_1\", \"column_name_2\"],\n",
    "# }\n",
    "\n",
    "# Network Isolation mode; default is off.\n",
    "enable_network_isolation = False\n",
    "\n",
    "# List of tags to be passed to the processing job.\n",
    "user_tags = []\n",
    "\n",
    "# Output configuration used as processing job container arguments. Only applies when writing to S3.\n",
    "# Uncomment to set additional configurations.\n",
    "output_config = {\n",
    "    output_name: {\n",
    "        \"content_type\": output_content_type,\n",
    "        # \"delimiter\": delimiter,\n",
    "        # \"compression\": compression,\n",
    "        # \"partition_config\": partition_config,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Refit configuration determines whether Data Wrangler refits the trainable parameters on the entire dataset. \n",
    "# When True, the processing job relearns the parameters and outputs a new flow file.\n",
    "# You can specify the name of the output flow file under 'output_flow'.\n",
    "# Note: There are length constraints on the container arguments (max 256 characters).\n",
    "refit_trained_params = {\n",
    "    \"refit\": False,\n",
    "    \"output_flow\": f\"data-wrangler-flow-processing-{flow_export_id}.flow\"\n",
    "}\n",
    "\n",
    "# KMS key for per object encryption; default is None.\n",
    "kms_key = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Configure Spark Cluster Driver Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Spark memory configuration. Change to specify the driver and executor memory in MB for the Spark cluster during processing.\n",
    "driver_memory_in_mb = 2048\n",
    "executor_memory_in_mb = 55742\n",
    "\n",
    "config = json.dumps({\n",
    "    \"Classification\": \"spark-defaults\",\n",
    "    \"Properties\": {\n",
    "        \"spark.driver.memory\": f\"{driver_memory_in_mb}m\",\n",
    "        \"spark.executor.memory\": f\"{executor_memory_in_mb}m\"\n",
    "    }\n",
    "})\n",
    "\n",
    "config_file = f\"config-{flow_export_id}.json\"\n",
    "with open(config_file, \"w\") as f:\n",
    "    f.write(config)\n",
    "\n",
    "config_s3_path = f\"spark_configuration/{processing_job_name}/configuration.json\"\n",
    "config_s3_uri = f\"s3://{bucket}/{config_s3_path}\"\n",
    "s3_client.upload_file(config_file, bucket, config_s3_path, ExtraArgs={\"ServerSideEncryption\": \"aws:kms\"})\n",
    "print(f\"Spark Config file uploaded to {config_s3_uri}\")\n",
    "os.remove(config_file)\n",
    "\n",
    "# Provides the spark config file to processing job and set the cluster driver memory. Uncomment to set.\n",
    "# data_sources.append(ProcessingInput(\n",
    "#     source=config_s3_uri,\n",
    "#     destination=\"/opt/ml/processing/input/conf\",\n",
    "#     input_name=\"spark-config\",\n",
    "#     s3_data_type=\"S3Prefix\",\n",
    "#     s3_input_mode=\"File\",\n",
    "#     s3_data_distribution_type=\"FullyReplicated\"\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Processer for the Pipeline\n",
    "\n",
    "Create a Processor object. The pipeline uses the processor to apply the transformations from your flow file to the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup processing job network configuration\n",
    "from sagemaker.network import NetworkConfig\n",
    "\n",
    "network_config = NetworkConfig(\n",
    "        enable_network_isolation=enable_network_isolation,\n",
    "        security_group_ids=None,\n",
    "        subnets=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor\n",
    "\n",
    "processor = Processor(\n",
    "    role=iam_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    volume_size_in_gb=volume_size_in_gb,\n",
    "    network_config=network_config,\n",
    "    sagemaker_session=sess,\n",
    "    output_kms_key=kms_key,\n",
    "    tags=user_tags\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SageMaker Pipeline\n",
    "A SageMaker pipeline is composed of a series of steps. You begin by creating a processing step to \n",
    "transform your data. If youâ€™re using the notebook to train a model, you also define a training step. \n",
    "\n",
    "## Define Pipeline Steps\n",
    "To create a SageMaker pipeline, create a `ProcessingStep` using the Data Wrangler processor defined \n",
    "in the preceding section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "data_wrangler_step = ProcessingStep(\n",
    "    name=\"DataWranglerProcessingStep\",\n",
    "    processor=processor,\n",
    "    inputs=[flow_input] + data_sources, \n",
    "    outputs=[processing_job_output],\n",
    "    job_arguments=[f\"--output-config '{json.dumps(output_config)}'\"] \n",
    "        + [f\"--refit-trained-params '{json.dumps(refit_trained_params)}'\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add a `TrainingStep` to the pipeline. The step trains a model on the transformed dataset. You \n",
    "can choose either an AutoML training step from SageMaker Autopilot or an XGBoost training step. By \n",
    "default, the AutoML training step is selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_training_step = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the AutoML training step, run the following cells as-is. To use the XGBoost training step, set `use_automl_step` to False.\n",
    "\n",
    "Currently, the AutoML training step only supports the `ENSEMBLING` mode. For more information about the AutoML training step, \n",
    "see [Pipeline Steps](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-automl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_automl_step = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For AutoML, specifying the target column of your dataset is required. \n",
    "The target column is the column that the model is trained to predict. \n",
    "Provide the name of the target column in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_attribute_name = \"\"  # Provide the target column name here\n",
    "\n",
    "if use_automl_step and not target_attribute_name:\n",
    "    raise RuntimeError(\"You must specify the target column name.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if add_training_step and use_automl_step:\n",
    "    from sagemaker import AutoML, AutoMLInput\n",
    "    from sagemaker.workflow.automl_step import AutoMLStep\n",
    "    from sagemaker.workflow.functions import Join\n",
    "    from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "    pipeline_session = PipelineSession()\n",
    "\n",
    "    training_input_content_type = None\n",
    "\n",
    "    if output_content_type == \"CSV\":\n",
    "        training_input_content_type = 'text/csv;header=present'\n",
    "    elif output_content_type == \"Parquet\":\n",
    "        training_input_content_type = 'x-application/vnd.amazon+parquet'\n",
    "\n",
    "    auto_ml = AutoML(\n",
    "        role=iam_role,\n",
    "        target_attribute_name=target_attribute_name,\n",
    "        sagemaker_session=pipeline_session,\n",
    "        mode=\"ENSEMBLING\"\n",
    "    )\n",
    "\n",
    "    s3_input = Join(\n",
    "        on=\"/\",\n",
    "        values=[\n",
    "            data_wrangler_step.properties.ProcessingOutputConfig.Outputs[output_name].S3Output.S3Uri,\n",
    "            data_wrangler_step.properties.ProcessingJobName,\n",
    "            f'{output_name.replace(\".\", \"/\")}',\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_args = auto_ml.fit(\n",
    "        inputs=AutoMLInput(\n",
    "            inputs=s3_input,\n",
    "            content_type=training_input_content_type,\n",
    "            target_attribute_name=target_attribute_name\n",
    "        )\n",
    "    )\n",
    "\n",
    "    training_step = AutoMLStep(\n",
    "        name=\"DataWrangerAutoML\",\n",
    "        step_args=train_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBoost algorithm uses the first column as the target column. If this is not the case for \n",
    "the data that youâ€™ve processed, use the \"Move column\" transform to make the target column the \n",
    "first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if add_training_step and not use_automl_step:\n",
    "    import boto3\n",
    "    from sagemaker.estimator import Estimator\n",
    "    from sagemaker.workflow.functions import Join\n",
    "\n",
    "    region = boto3.Session().region_name\n",
    "\n",
    "    image_uri = sagemaker.image_uris.retrieve(\n",
    "        framework=\"xgboost\",\n",
    "        region=region,\n",
    "        version=\"1.5-1\",\n",
    "        py_version=\"py3\",\n",
    "        instance_type=instance_type,\n",
    "    )\n",
    "    xgb_train = Estimator(\n",
    "        image_uri=image_uri,\n",
    "        instance_type=instance_type,\n",
    "        instance_count=1,\n",
    "        role=iam_role,\n",
    "    )\n",
    "    xgb_train.set_hyperparameters(\n",
    "        objective=\"reg:squarederror\",\n",
    "        num_round=3,\n",
    "    )\n",
    "\n",
    "    from sagemaker.inputs import TrainingInput\n",
    "    from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "    xgb_input_content_type = None\n",
    "\n",
    "    if output_content_type == \"CSV\":\n",
    "        xgb_input_content_type = 'text/csv'\n",
    "    elif output_content_type == \"Parquet\":\n",
    "        xgb_input_content_type = 'application/x-parquet'\n",
    "\n",
    "    training_step = TrainingStep(\n",
    "        name=\"DataWrangerTrain\",\n",
    "        estimator=xgb_train,\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=Join(\n",
    "                    on=\"/\",\n",
    "                    values=[\n",
    "                        data_wrangler_step.properties.ProcessingOutputConfig.Outputs[output_name].S3Output.S3Uri,\n",
    "                        data_wrangler_step.properties.ProcessingJobName,\n",
    "                        f'{output_name.replace(\".\", \"/\")}',\n",
    "                    ]\n",
    "                ),\n",
    "                content_type=xgb_input_content_type\n",
    "            )\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Pipeline of Parameters, Steps\n",
    "Now you will create the SageMaker pipeline that combines the steps created above so it can be executed. \n",
    "\n",
    "Define Pipeline parameters that you can use to parametrize the pipeline. Parameters enable custom pipeline executions and schedules without having to modify the Pipeline definition.\n",
    "\n",
    "The parameters supported in this notebook includes:\n",
    "\n",
    "- `instance_type` - The ml.* instance type of the processing job.\n",
    "- `instance_count` - The instance count of the processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "# Define Pipeline Parameters\n",
    "instance_type = ParameterString(name=\"InstanceType\", default_value=\"ml.m5.4xlarge\")\n",
    "instance_count = ParameterInteger(name=\"InstanceCount\", default_value=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will create a pipeline with the steps and parameters defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import uuid\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# Create a unique pipeline name with flow export name\n",
    "pipeline_name = f\"pipeline-{flow_export_name}\"\n",
    "\n",
    "# Combine pipeline steps\n",
    "pipeline_steps = [data_wrangler_step]\n",
    "if add_training_step:\n",
    "    pipeline_steps.append(training_step)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[instance_type, instance_count],\n",
    "    steps=pipeline_steps,\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Examining the pipeline definition\n",
    "\n",
    "The JSON of the pipeline definition can be examined to confirm the pipeline is well-defined and \n",
    "the parameters and step properties resolve correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the pipeline to SageMaker and start execution\n",
    "\n",
    "Submit the pipeline definition to the SageMaker Pipeline service and start an execution. The role passed in \n",
    "will be used by the Pipeline service to create all the jobs defined in the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=iam_role)\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Operations: Examine and Wait for Pipeline Execution\n",
    "\n",
    "Describe the pipeline execution and wait for its completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Waiter will wait for up to 1 hour; increase the delay and max_attempts if necessary\n",
    "    execution.wait(delay=30, max_attempts=120)\n",
    "except Exception as e:\n",
    "    listed_steps = execution.list_steps()\n",
    "    errors = []\n",
    "    for step in listed_steps:\n",
    "        if \"FailureReason\" in step:\n",
    "            errors.append(step[\"FailureReason\"])\n",
    "    raise RuntimeError(str(errors)) from e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the steps in the execution. These are the steps in the pipeline that have been resolved by the step \n",
    "executor service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the pipeline execution status and details in Studio. For details please refer to \n",
    "[View, Track, and Execute SageMaker Pipelines in SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-studio.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangler Step S3 Output Location\n",
    "The output of your Data Wrangler processing step will be printed below. To prevent data of different processing jobs \n",
    "and different output nodes from being overwritten or combined, Data Wrangler uses the name of the processing job and \n",
    "the name of the output to write the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingJob\n",
    "\n",
    "data_wrangler_job_arn = execution.list_steps()[-1][\"Metadata\"][\"ProcessingJob\"][\"Arn\"]\n",
    "data_wrangler_job = ProcessingJob.from_processing_arn(sess, data_wrangler_job_arn)\n",
    "data_wrangler_job_name = data_wrangler_job.describe()[\"ProcessingJobName\"]\n",
    "\n",
    "s3_job_results_path = f\"{s3_output_base_path}/{data_wrangler_job_name}/{output_name.replace('.', '/')}\"\n",
    "print(f\"Job results are saved to S3 path: {s3_job_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SageMaker Inference Pipeline\n",
    "\n",
    "## Set Target Column\n",
    "If the dataset that you used to create the Data Wrangler flow has a target column, you must specify \n",
    "it as the value of `target_column_name`. Set `contains_target_column` to False if you're not using \n",
    "a target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byo_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contains_target_column = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if byo_model or not use_automl_step:\n",
    "    target_column_name = \"\"  # Provide the target column name here\n",
    "else:\n",
    "    target_column_name = target_attribute_name\n",
    "\n",
    "if contains_target_column and not target_column_name:\n",
    "    raise RuntimeError(\"You must specify the target column name.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Locations\n",
    "Data Wrangler has created a modified version of your flow file to make it usable for inference. \n",
    "The following notebook cells upload the modified Data Wrangler flow to Amazon S3 and get the S3 \n",
    "location of the model that youâ€™ve trained. If youâ€™re using your own model, specify its S3 \n",
    "location as the value of algo_model_uri and  set byo_model to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if byo_model:\n",
    "    algo_model_uri = \"\"  # Provide your model S3 URI here\n",
    "elif use_automl_step:\n",
    "    automl_job_arn = execution.list_steps()[0][\"Metadata\"][\"AutoMLJob\"][\"Arn\"]\n",
    "    automl_job_name = automl_job_arn.split(\"/\")[1]\n",
    "    automl_job_desc = sess.describe_auto_ml_job(automl_job_name)\n",
    "    best_inference_container = automl_job_desc[\"BestCandidate\"][\"InferenceContainers\"][0]\n",
    "    algo_model_uri = best_inference_container[\"ModelDataUrl\"]\n",
    "else:\n",
    "    training_job_arn = execution.list_steps()[0][\"Metadata\"][\"TrainingJob\"][\"Arn\"]\n",
    "    training_job_name = training_job_arn.split(\"/\")[1]\n",
    "    training_job_desc = sess.describe_training_job(training_job_name)\n",
    "    algo_model_uri = training_job_desc[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "print(f\"Model URI: {algo_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Sagemaker session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# You can configure this with your own bucket name, e.g.\n",
    "# inference_bucket = \"my-bucket\"\n",
    "inference_bucket = sess.default_bucket()\n",
    "\n",
    "print(f\"Data Wrangler export storage bucket: {inference_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "# name of the inference flow model which should exist in the current notebook working directory\n",
    "inference_flow_name = \"loan-default-inf-pipeline-DO-NOT-DEL_2022-11-19-01-36-10.tar.gz\"\n",
    "inference_flow_path = f\"/home/sagemaker-user/data/data_wrangler_inference_flows/{inference_flow_name}\"\n",
    "\n",
    "# Upload inference flow model to S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(inference_flow_path, inference_bucket, f\"data_wrangler_inference_flows/{inference_flow_name}\", ExtraArgs={\"ServerSideEncryption\": \"aws:kms\"})\n",
    "\n",
    "inference_flow_uri = f\"s3://{inference_bucket}/data_wrangler_inference_flows/{inference_flow_name}\"\n",
    "\n",
    "print(f\"Data Wrangler inference flow model URI: {inference_flow_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Models\n",
    "The following cells use the model locations that youâ€™ve specified to create models in SageMaker. \n",
    "You can also include additional models in the inference pipeline. To include additional models, \n",
    "define them in the following cells and append them to `pipeline_models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.strftime(datetime.now(), '%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "# IAM role for the models.\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "dw_container_uri = \"174368400705.dkr.ecr.us-west-2.amazonaws.com/sagemaker-data-wrangler-container:1.x\"\n",
    "\n",
    "if byo_model:\n",
    "    algo_container_uri = \"\"  # Provide the ECR image of the algorithm used to train your model\n",
    "elif use_automl_step:\n",
    "    algo_container_uri = best_inference_container[\"Image\"]\n",
    "else:\n",
    "    algo_container_uri = image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines and appends the data_wrangler_model. The Data Wrangler model is \n",
    "the data processing step in the inference pipeline that youâ€™re creating. It uses the \n",
    "transformations that youâ€™ve made in the Data Wrangler flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "data_wrangler_model_name = f\"DataWranglerInferencePipelineFlowModel-{timestamp}\"\n",
    "\n",
    "data_wrangler_model = Model(\n",
    "    image_uri=dw_container_uri,\n",
    "    model_data=inference_flow_uri,\n",
    "    role=iam_role,\n",
    "    name=data_wrangler_model_name,\n",
    "    sagemaker_session=sess,\n",
    "    env={\"INFERENCE_TARGET_COLUMN_NAME\": target_column_name}\n",
    ")\n",
    "\n",
    "pipeline_models.append(data_wrangler_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines and appends the algo_model.  The algorithm model is the predictive \n",
    "step in the inference pipeline. Itâ€™s either the model that youâ€™ve trained in the notebook or \n",
    "a model that youâ€™ve trained elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_model_name = f\"DataWranglerInferencePipelineAlgoModel-{timestamp}\"\n",
    "algo_environment = None  # Provide your model environment variables here if bringing your own model\n",
    "\n",
    "if not byo_model and use_automl_step:\n",
    "    algo_environment = best_inference_container[\"Environment\"]\n",
    "\n",
    "algo_model = Model(\n",
    "    image_uri=algo_container_uri,\n",
    "    model_data=algo_model_uri,\n",
    "    role=iam_role,\n",
    "    name=algo_model_name,\n",
    "    sagemaker_session=sess,\n",
    "    env=algo_environment,\n",
    ")\n",
    "\n",
    "pipeline_models.append(algo_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Inference Pipeline\n",
    "Use the models that youâ€™ve defined to create a PipelineModel. The pipeline model passes the \n",
    "intermediate output of the Data Wrangler model to the algorithm model.\n",
    "\n",
    "For models, the order of the list of models is the order of the pipeline. The pipeline wonâ€™t \n",
    "work if the Data Wrangler model isnâ€™t first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pipeline import PipelineModel\n",
    "\n",
    "inference_pipeline_model_name = f\"DataWranglerInferencePipelineModel-{timestamp}\"\n",
    "\n",
    "inference_pipeline_model = PipelineModel(\n",
    "    models=pipeline_models,\n",
    "    role=iam_role,\n",
    "    name=inference_pipeline_model_name,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells deploy the pipeline model to an endpoint. You must deploy the pipeline \n",
    "model to an endpoint before you can make inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the endpoint configuration as needed\n",
    "instance_type = \"ml.m5.xlarge\"\n",
    "instance_count = 1\n",
    "\n",
    "endpoint_name = f\"DataWranglerInferencePipelineEndpoint-{timestamp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline_model.deploy(instance_count, instance_type, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke the Inference Pipeline with a Predictor\n",
    "\n",
    "## Create a Predictor\n",
    "Create a predictor to make requests to your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Sample Inference Request\n",
    "The following cells use a data point from your dataset to get a prediction from the endpoint. \n",
    "The data point doesnâ€™t have any transformations from your Data Wrangler flow applied to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_record_csv = '''loan_status,loan_amount,funded_amount_by_investors,loan_term,interest_rate,installment,grade,sub_grade,verification_status,issued_on,purpose,dti,earliest_credit_line,inquiries_last_6_months,open_credit_lines,derogatory_public_records,revolving_line_utilization_rate,total_credit_lines,employer_title,home_ownership,employment_length,annual_income\n",
    "fully paid,5000,4975.0,36,10.65,162.87,b,b2,verified,2011-12-01,credit_card,27.65,1985-01-01,1,3,0,83.7,9,,rent,10.0,24000.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "sample_record_io = StringIO(sample_record_csv)\n",
    "sample_df = pd.read_csv(sample_record_io)\n",
    "\n",
    "if contains_target_column:\n",
    "    # Drop the target column from the sample csv\n",
    "    print(f\"Target column value of sample record: {sample_df.iloc[0][target_column_name]}\")\n",
    "    sample_df = sample_df.drop(columns=[target_column_name])\n",
    "\n",
    "sample_record_payload = sample_df.to_csv(header=False, index=False)\n",
    "print(f\"Sample record to predict: {sample_record_payload}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictor.predict(sample_record_payload, initial_args={\"ContentType\": \"text/csv\"})\n",
    "\n",
    "print(f\"The predicted target is: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Cleanup\n",
    "To delete the SageMaker resources created in this notebook, set `resource_deletion` to True.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_deletion = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resource_deletion:\n",
    "    pipeline.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resource_deletion:\n",
    "    predictor.delete_endpoint()\n",
    "    inference_pipeline_model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
